env:
  name: MBRLHalfCheetah-v0
  reward_func: environments.half_cheetah.HalfCheetahEnv.get_reward_torch
  state_transformer: environments.cartpole.CartpoleEnv.identity
  render: false
  instances: 1

num_trials: 20
trial_timesteps: 200
device: cuda
random_seed: 100
log_cfg: conf/logging.yaml
log_dir: logs
log_trajectories: false

dynamics_model:
  type: P
  jit: true
  P:
    clazz: models.pnn.PNN
    loss:
      clazz: models.loss.NLLLoss

  ensemble_size: 5


training:
  # Incrementally update the model between trials.
  # if false, model will be trained from scratch for training.full_epochs at every trial
  # if true, model will trained for training.full_epochs in the first trial (from motor babbling),
  # and for subsequent trials it will be updated for training.incremental_epochs
  incremental: false
  full_epochs: 100
  incremental_epochs: 5

  batch_size: 16

  optimizer:
    clazz: torch.optim.Adam
    params:
      lr: 1e-4

  testing:
    # ratio between test and train splits, 0.7 means 70% in train and 30% in test.
    # to use all the data for training and not perform testing set to 1
    split: 0.9


mpc:
  planning_horizon: &planning_horizon 25
  optimizer:
    clazz: optimizers.RandomShootingOptimizer
    params:
      planning_horizon: *planning_horizon
      num_trajectories: 2000
